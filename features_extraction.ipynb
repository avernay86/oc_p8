{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import findspark\n",
    "#findspark.init()\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "key_id = os.getenv('key_id')\n",
    "secret = os.getenv('secret')\n",
    "\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell\"\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=com.amazonaws:aws-java-sdk-bundle:1.11.271,org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell\"\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell\"\n",
    "\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext.getOrCreate()\n",
    "hadoop_conf=sc._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "hadoop_conf.set(\"fs.s3a.awsAccessKeyId\", key_id)\n",
    "hadoop_conf.set(\"fs.s3a.awsSecretAccessKey\", secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet, preprocess_input\n",
    "#from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************done********\n",
      "DataFrame[value: string]\n",
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'flatMap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1f98d4cd756d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m features_df = image_df.select('path',\n\u001b[0;32m---> 84\u001b[0;31m                               \u001b[0mget_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'path'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                               featurize_udf('content').alias('features'))\n\u001b[1;32m     86\u001b[0m \u001b[0;31m#features_df.write.mode(\"overwrite\").csv(\"sample_file.csv\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-1f98d4cd756d>\u001b[0m in \u001b[0;36mget_filename\u001b[0;34m(lines)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mseparates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseparates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'flatMap'"
     ]
    }
   ],
   "source": [
    "sample_img_dir = 's3a://oc-av-p8/images'\n",
    "text = 's3a://oc-av-p8test/text.txt'\n",
    "text_df = spark.read.text(text)\n",
    "print('************done********')\n",
    "print(text_df)\n",
    "text_df.show\n",
    "\n",
    "#sample_img_dir = 'data/images/'\n",
    "image_df = spark.read.format(\"binaryFile\") \\\n",
    "            .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "            .option(\"recursiveFileLookup\", \"true\") \\\n",
    "            .load(sample_img_dir)\n",
    "\n",
    "#pkeys = sc.parallelize(keys)\n",
    "    # Call the map step to handle reading in the file contents\n",
    "#activation = pkeys.flatMap(map_func)\n",
    "\n",
    "model = MobileNet(include_top=False)\n",
    "bc_model_weights = sc.broadcast(model.get_weights())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "fonctions adapt√©es de la documentation databricks:\n",
    "https://docs.databricks.com/applications/machine-learning/preprocess-data/transfer-learning-tensorflow.html\n",
    "\"\"\"\n",
    "\n",
    "def model_fn():\n",
    "  \"\"\"\n",
    "  Returns a MobileNet model with top layer removed and broadcasted pretrained weights.\n",
    "  \"\"\"\n",
    "  model = MobileNet(weights=None,\n",
    "   #input_shape=(100,100,3), \n",
    "   pooling='avg',\n",
    "   include_top=False)\n",
    "  model.set_weights(bc_model_weights.value)\n",
    "  return model\n",
    "\n",
    "def preprocess(content):\n",
    "  \"\"\"\n",
    "  Preprocesses raw image bytes for prediction.\n",
    "  \"\"\"\n",
    "  img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "  arr = img_to_array(img)\n",
    "  arr = tf.convert_to_tensor(arr)\n",
    "  return preprocess_input(arr)\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "  \"\"\"\n",
    "  Featurize a pd.Series of raw images using the input model.\n",
    "  :return: a pd.Series of image features\n",
    "  \"\"\"\n",
    "  print('*************content_series*********', content_series)\n",
    "  input = tf.stack(content_series.map(preprocess))\n",
    "  print('input shape', input.shape)\n",
    "  preds = model.predict(input)\n",
    "  # For some layers, output features will be multi-dimensional tensors.\n",
    "  # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "  output = [p.flatten() for p in preds]\n",
    "  print('output shape:', np.shape(output))\n",
    "  output = pd.Series(output)\n",
    "  return output\n",
    "\n",
    "@pandas_udf('array<float>')\n",
    "def featurize_udf(content_series_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
    "  '''\n",
    "  This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "  The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "  \n",
    "  :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "  '''\n",
    "  # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "  # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "  model = model_fn()\n",
    "  for content_series in content_series_iter:\n",
    "    yield featurize_series(model, content_series)\n",
    "    \n",
    "def get_filename(lines):\n",
    "    separates = lines.flatMap(lambda line: line.split('/')[-2:])\n",
    "    filename = separates.map(lambda sep: '/'.join(sep))\n",
    "    return filename\n",
    "\n",
    "features_df = image_df.select('path',\n",
    "                              get_filename('path').alias('filename'), \n",
    "                              featurize_udf('content').alias('features'))\n",
    "#features_df.write.mode(\"overwrite\").csv(\"sample_file.csv\")\n",
    "#features_df.write.mode(\"overwrite\").parquet('s3a://oc-av-p8/parquets')\n",
    "\n",
    "print(image_df)\n",
    "#image_df.select(\"image.origin\", \"image.width\", \"image.height\").show(truncate=False)\n",
    "\n",
    "print(features_df)\n",
    "features_df.printSchema()\n",
    "features_df.select('filename').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
